%% LyX 2.0.6 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{scrartcl}
\renewcommand{\sfdefault}{cmss}
\renewcommand{\familydefault}{\sfdefault}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{listings}
\lstset{backgroundcolor={\color{lightgray}},
basicstyle={\ttfamily},
breaklines=true,
columns=fullflexible}
\usepackage[a4paper]{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=2cm,rmargin=2cm}
\usepackage{fancyhdr}
\pagestyle{fancy}
\setlength{\parskip}{\medskipamount}
\setlength{\parindent}{0pt}
\usepackage{color}
\usepackage{babel}
\usepackage{amsmath}
\usepackage[unicode=true,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=false,
 breaklinks=true,pdfborder={0 0 0},backref=page,colorlinks=false]
 {hyperref}
\hypersetup{pdftitle={SeisSol HDF5},
 pdfauthor={Stefan Wenk},
 pdfsubject={Implementation of HDF5 support into SeisSol},
 pdfkeywords={Discontinuous Galerkin, HDF5}}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\numberwithin{table}{section}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{dashbox}
% break url
\usepackage{breakurl}
% TIKZ
\usepackage{tikz}
\newcount\dirtree@lvl
\newcount\dirtree@plvl
\newcount\dirtree@clvl
\def\dirtree@growth{%
  \ifnum\tikznumberofcurrentchild=1\relax
  \global\advance\dirtree@plvl by 1
  \expandafter\xdef\csname dirtree@p@\the\dirtree@plvl\endcsname{\the\dirtree@lvl}
  \fi
  \global\advance\dirtree@lvl by 1\relax
  \dirtree@clvl=\dirtree@lvl
  \advance\dirtree@clvl by -\csname dirtree@p@\the\dirtree@plvl\endcsname
  \pgf@xa=1cm\relax
  \pgf@ya=-1cm\relax
  \pgf@ya=\dirtree@clvl\pgf@ya
  \pgftransformshift{\pgfqpoint{\the\pgf@xa}{\the\pgf@ya}}%
  \ifnum\tikznumberofcurrentchild=\tikznumberofchildren
  \global\advance\dirtree@plvl by -1
  \fi
}
\tikzset{
  dirtree/.style={
    growth function=\dirtree@growth,
    every node/.style={anchor=north,draw=black,thick},
    every child node/.style={anchor=west},
    edge from parent path={(\tikzparentnode\tikzparentanchor) |- (\tikzchildnode\tikzchildanchor)}
  }
}

\AtBeginDocument{
  \def\labelitemi{\(\cdot\)}
}

\makeatother

\begin{document}

\title{Parallel IO in SeisSol using the HDF5 library}


\author{Stefan Wenk}

\maketitle
\newpage{}


\section{What do we need?}
\begin{itemize}
\item wavefield snapshots in time on whole simulation domain (snapshots)
\item wavefield snapshots in time on fault (fault output)
\item wavefield snapshot in space over time (receiver output)
\item wavefield snapshot on fault over time (fault receiver output)
\item save metadata:

\begin{itemize}
\item information on code version (svn revision, host, architecture, compilers,
...)
\item information on simulation (time step, Courant number, material model,
source type, source/receiver location, ...)
\end{itemize}
\item add postprocessing datasets
\item add postprocessing metadata:

\begin{itemize}
\item applied filter 
\item applied parameters
\end{itemize}
\end{itemize}

\section{Why HDF5?}
\begin{itemize}
\item parallel IO for large scale jobs
\item portable file format
\item handle complex data objects and metadata
\item available postprocessing tools
\item available on supercomputing facilities
\item high-performance
\item large user community
\item frequent updates
\end{itemize}
\newpage{}


\section{The HDF5 library}


\subsection{General}

HDF5 files can be organized by objects, i.e. GROUPS, DATASETS and
ATTRIBUTES. Any HDF5 group or dataset may have an associated attribute
list. 

In order to maximize performance, the HDF5 library provides ways to
specify how the data is stored on disk, how it is accessed, and how
it should be held in memory.

Datasets in HDF5 can represent arrays with any number of dimensions
(up to 32). However, in the file this dataset must be stored as part
of the 1-dimensional stream of data that is the file. The way in which
the multidimensional dataset is mapped to the serial file is called
the layout. The most obvious way to accomplish this is to simply flatten
the dataset in a way similar to how arrays are stored in memory, serializing
the entire dataset into a block on disk, which maps directly to a
memory buffer the size of the dataset. This is called a contiguous
layout.

An alternative to the contiguous layout is the chunked layout. Whereas
contiguous datasets are stored in a single block in the file, chunked
datasets are split into multiple chunks which are all stored separately
in the file. The chunks can be stored in any order and any position
within the HDF5 file. Chunks can then be read and written individually,
improving performance when operating on a subset of the dataset. 

In case of parallel IO in addition hyperslabs have to be selected
to organize the parallel file access. The programming model for writing
and reading hyperslabs is:
\begin{itemize}
\item Each process defines the memory and file hyperslabs. 
\item Each process executes a partial write/read call which is either collective
or independent.
\end{itemize}
The memory and file hyperslabs in the first step are defined with
the function h5sselect\_hyperslab\_f. The start (or offset), count,
stride, and block parameters define the portion of the dataset to
write to. By changing the values of these parameters you can write
hyperslabs with Parallel HDF5 by contiguous hyperslab, by regularly
spaced data in a column/row, by patterns, and by chunks.


\subsection{Compilation}

The library itself is implemented in C. It has to be compiled with
a fortran binding. To run with SeisSol shared libraries have to be
generated. The szip and zlib external libraries are optional to use
data compression with HDF5. 

General installation instructions using Intel compilers can be found
here: \href{http://software.intel.com/en-us/articles/performance-tools-for-software-developers-building-hdf5-with-intel-compilers}{http://software.intel.com/en-us/articles/performance-tools-for-software-developers-building-hdf5-with-intel-compilers}


\subsubsection{Set up environment}

At LMU Geophysics institute, load compiler variables:

\begin{lstlisting}
source /opt/intel/composerxe/bin/compilervars.sh intel64 
source /opt/intel/impi/4.1/bin64/mpivars.sh 
export OMPI_CXX=/opt/intel/composerxe/bin/icpc 
export OMPI_FC=/opt/intel/composerxe/bin/ifort
\end{lstlisting}



\subsubsection{Install szip}

szip compression software, providing lossless compression of scientific
data, has been provided with HDF software products. Szip is a stand-alone
library that is configured as an optional filter in HDF5. Depending
on which Szip library is used (encoder enabled or decode-only), an
HDF5 application can create, write, and read datasets compressed with
Szip compression, or can only read datasets compressed with Szip.

Source from: \href{http://www.hdfgroup.org/doc_resource/SZIP/}{http://www.hdfgroup.org/doc\_{}resource/SZIP/}

export environment variables:

\begin{lstlisting}
export CC=icc 
export CXX=icpc 
export FC=ifort 
export CFLAGS="-O3 -xHost -ip"
export CXXFLAGS="-O3 -xHost -ip"
export FCFLAGS="-O3 -xHost -ip"
\end{lstlisting}


extract package and go to folder:

\begin{lstlisting}
tar -zxvf szip-2.1.tar.gz 
cd szip-2.1
\end{lstlisting}


configure and make:

\begin{lstlisting}
./configure --prefix=/home/wenk 
make 
make check 
make install
\end{lstlisting}



\subsubsection{Install zlib}

zlib is designed to be a free, general-purpose, lossless data-compression
library for use on any computer hardware and operating system. Source
from: \href{http://www.zlib.net/}{http://www.zlib.net/}

set up environment:

\begin{lstlisting}
export CC=icc
export CFLAGS="-O3 -xHost -ip"
\end{lstlisting}


extract package and go to folder:

\begin{lstlisting}
tar -zxvf zlib-1.2.7.tar.gz
cd zlib-1.2.7
\end{lstlisting}


configure and make:

\begin{lstlisting}
./configure --prefix=/home/wenk 
make 
make check 
make install
\end{lstlisting}



\subsubsection{Install HDF5}

Source from: \href{http://www.hdfgroup.org/}{http://www.hdfgroup.org/}

Configure example with zlib compression, fortran binding and shared
library options:

\begin{lstlisting}
CC=mpiicc FC=mpiifort ./configure --prefix=/home/wenk --with-zlib=/home/wenk/lib --enable-parallel --enable-fortran --enable-shared
\end{lstlisting}


make HDF5:

\begin{lstlisting}
make 
make check (probably errors occur. see known errors below)
make install
\end{lstlisting}


Known errors due to configure script defaults:

\begin{lstlisting}
# wrong path
AM_LDFLAG = $(prefix)/lib/lib

# exchange with mpirun -np
RUNEXEC = mpiexec -n
\end{lstlisting}



\subsubsection{Compile with HDF5}

Makefile example:

\begin{lstlisting}
# EXPORT hdf5 install folder as path to shared hdf5 libs 
export LD_LIBRARY_PATH=/home/wenk/lib:$LD_LIBRARY_PATH

# set up environment
LIB_HDF = -L/home/wenk/lib -lhdf5 -lhdf5_fortran 
INC_HDF = -I/home/wenk/include

CC = mpiicc 
FC = mpiifort 
FFLAGS = -fpp -r8 -O0 -g -traceback -check all -ftz -align -fno-alias 
CPPFLAGS =  -g -O0 -ftz -align -fno-alias -DgFortran

SRC = your_source_files.f90

$(OBJS):
	$(FC) $(FFLAGS) -c $(INC_HDF) $(SRC)
$(PROG): $(OBJS)
	$(FC) $(FFLAGS) $(LIB_HDF) $(OBJS) -o $(PROG)
\end{lstlisting}


\newpage{}


\section{Using HDF5 in SeisSol}

The implementation is separated in an initialization and a processing
phase. During initialization the output processes are selected and
the structure of the HDF5 file is set up. In the output phase, the
library is called to write the data at the predefined positions in
the file.


\subsection{Initialization of MPI communication}

In case of a parallel file access, the processes participating in
the IO operation have to be known. In contrast to a wavefield snapshot
in time, a time series in space will be output by one single process.
Depending on the selected receiver locations, multiple processes participate
in the IO operation. To allow for communication of selected processes
when the HDF5 library is called, an MPI sub-communicator has to be
generated. The sub-processes are selected from the global MPI\_COMM\_WORLD
communicator and included into a new MPI group. Within this group
a new communicator can be created.

\begin{lstlisting}
! generate new mpi communicator for all processes with receivers     
! find group of mpi comm world communicator     
CALL MPI_COMM_GROUP(MPI_COMM_WORLD, world_grp, MPI%iErr)     
! put selected ranks (with receivers) of global group in a new subgroup     
CALL MPI_GROUP_INCL(world_grp, nranks, rec_ranks, rec_grp, MPI%iErr)     
! create communicator for new subgroup     
CALL MPI_COMM_CREATE(MPI_COMM_WORLD, rec_grp, rec_comm, MPI%iErr)
\end{lstlisting}



\subsection{Initialization of the HDF5 file}

The HDF5 file is initilized:
\begin{itemize}
\item to set up dataset dimensions,
\item to set up data buffers
\item to initialize an offset variable
\item to write file dataset and group attributes,
\item to write datasets which do not change in the time loop (receiver location),
and 
\item to open up the file for parallel file access.
\end{itemize}
Two groups for receiver and source data are generated. The receiver
group is populated with up to five datasets (location, time, stress,
velocity, rotation) depending on the chosen input parameters. In any
case the location and time datasets are written. 

The dimension of each dataset is calculated to generate a proper dataspace
in the file. The file will be continuously filled by a data buffer.
The data buffer dimension of each process corresponds to the number
of local receivers, the number of components and $10\%$ of the total
number of samples. To keep track of the output level, an offset variable
is applied.

See the following sketch of the file structure. The HDF5 objects are
boxed where, \fcolorbox{red}{red!30}{groups} are in red, \fcolorbox{black}{white}{datasets}
in white, and \fcolorbox{gray}{gray!30}{attributes} in gray. The
objects in the dashed boxes are \dbox{not implemented} yet.

\tikzstyle{attrb}=[draw=gray,fill=gray!30]
\tikzstyle{group}=[draw=red,fill=red!30]
\tikzstyle{optional}=[dashed]
\begin{tikzpicture}[dirtree]
  \node[group]{HDF5-file}  
    child{node[attrb,optional]{(external) SVN revisoin, compiler, host name, architecture, ...}}
    child{node[group]{source}
          child{node[attrb,optional]{type, ...}}
          child{node[optional]{source time function}}
          child{node[optional]{location}}
          child{node[optional]{mechanism}}
         }
    child{node[group]{receiver}
          child{node[attrb]{nrec, nsamp, dt, tend, maxiter}}
          child{node{location}
                child{node[attrb]{components: x, y, z}}
                child{node{2D data [nrec$\times$ncomp]}}}
          child{node{time}
                child{node[attrb]{components: t}}
                child{node{2D data [nrec$\times$nsamp]}}}
          child{node{sress}
                child{node[attrb]{components: sxx, syy, szz, sxy, sxz, syz}}
                child{node{3D data [nrec$\times$nsamp$\times$ncomp]}}}
          child{node{velocity}
                child{node[attrb]{components: u, v, w}}
                child{node{3D data [nrec$\times$nsamp$\times$ncomp]}}}
          child{node{rotation}
                child{node[attrb]{components: rx, ry, rz}}
                child{node{3D data [nrec$\times$nsamp$\times$ncomp]}}}
          }
  ;
\end{tikzpicture}


\subsection{Processing}

The best IO performance of the HDF5 library can be achieved by writing
contiguous data of each process, and stream this data unorganized
to the datasets in the HDF5 file. But, to keep the ordering of the
receiver data in the dataspace of the file, an irregular hyperslab
selection layout has to be chosen.

Irregular dataspace can be selceted, by adding up multiple hyperslab
calls:

\begin{lstlisting}
DO i=1,nrec
  start_hdf = (/ start_r1(i), start_r2, 0 /)       
    IF (i.EQ.1) THEN         
      ! overwrite dataset selection         
      CALL h5sselect_hyperslab_f(dset_space_id,H5S_SELECT_SET_F,start_hdf,count_hdf,error)       
    ELSE         
      ! add new hyperslab         
      CALL h5sselect_hyperslab_f(dset_space_id,H5S_SELECT_OR_F,start_hdf,count_hdf,error)       
    ENDIF     
ENDDO
\end{lstlisting}


Writing the data can be performed in a collective or independent manner.
For collective IO each MPI process has to participate in the write
CALL. In the independent IO mode the MPI processes can separately
write the data:

\begin{lstlisting}
IF (io_flag.EQ.1) THEN       
  ! collective       
  CALL h5pset_dxpl_mpio_f(plist_id, H5FD_MPIO_COLLECTIVE_F, error)     
ELSE       
  ! independent       
  CALL h5pset_dxpl_mpio_f(plist_id, H5FD_MPIO_INDEPENDENT_F, error)     
ENDIF
\end{lstlisting}


\newpage{}


\section{Verification}


\subsection{Operational test}

Example is the Tetra\_LOH4 from the CRON folder with some changes:
\begin{itemize}
\item source: single double couple
\item mesh: homogeneous tetrahedral element size of 3000m (4786 elements).
\item receiver: location
\end{itemize}
Mesh partitioning into 4 domains. Receiver distribution:

\begin{tabular}{|c|c|}
\hline 
process & number of receivers\tabularnewline
\hline 
\hline 
0 & 1\tabularnewline
1 & 2\tabularnewline
2 & 6\tabularnewline
3 & 1\tabularnewline
\hline 
\end{tabular}

Revision 1106 is an old one. Revision 1445 still has the old receiver
routine but everything else is newest code. Revision New is the latest
HDF5 implementation with the common calculations of serial and parallel
receiver output and the collapsed gts and lts serial routine.

\begin{tabular}{|c|c|c|c|c|}
\hline 
Revision & Time stepping scheme & HDF5 & IO & total runtime in s\tabularnewline
\hline 
\hline 
1106 & local & no & serial & 9.67567499999859\tabularnewline
1445 & local & no & serial & 9.46342500000174\tabularnewline
New & local & no & serial & 9.44127500000468\tabularnewline
New  & local & yes & independent & 11.6623249999903\tabularnewline
1445 & global & no & serial & 10.7427749999988\tabularnewline
New  & global & yes & collective & 10.0697999999975\tabularnewline
New  & global & yes & independent & 10.0640500000009\tabularnewline
\hline 
\end{tabular}


\subsection{Performance analysis}
\end{document}
